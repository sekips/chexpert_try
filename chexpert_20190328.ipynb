{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheXpert触ってみた  \n",
    "## CheXpertデータセット  \n",
    "Stanford ML groupが公開した胸部レントゲン画像のデータセット。  \n",
    "2002年10月から2017年7月までの間にスタンフォード大学附属病院で65,240人の患者に対して撮影された224,316枚の胸部レントゲン写真(正面像・側面像を含む)。  \n",
    "放射線読影医の読影レポートに対してルールベースのテキスト処理を行い、14種類の診断項目のラベルをつけてある(マルチラベル)。  \n",
    "オリジナルのデータセットとして全体で439GBのものに加え、解像度を落としたデータセット(11GB程度)のものがダウンロード可能。  \n",
    "CheXpertのサイト:https://stanfordmlgroup.github.io/competitions/chexpert/  \n",
    "CheXpertの論文:https://arxiv.org/abs/1901.07031\n",
    "  \n",
    "### 解像度を落としたデータで中身をざっとみてみる  \n",
    "解像度を落としたデータを用い、側面像は取り除き、今回は正面像のみで解析を行った"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import scipy\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as multi\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def glance(path):\n",
    "    \"\"\"試しに写真を表示する用の関数\"\"\"\n",
    "    img = scipy.misc.imread(path)\n",
    "    plt.imshow(img)\n",
    "    plt.gray()\n",
    "    plt.show()\n",
    "    \n",
    "labels_train_raw = pd.read_csv(\"train.csv\")\n",
    "path = \"path/to\" #CheXpert-v1.0-smallのディレクトリのある場所へのフルパス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## それぞれのラベルのついた画像を数枚ずつ確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#No Finding \n",
    "print(\"異常所見なし\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[0,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[5,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[18,:][\"Path\"]))\n",
    "\n",
    "#Enlarged Cardiomediastinum \n",
    "print(\"縦隔陰影の拡大\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[13,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[55,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[147,:][\"Path\"]))\n",
    "\n",
    "#Cardiomegaly \n",
    "print(\"心拡大\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[132,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[133,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[134,:][\"Path\"]))\n",
    "\n",
    "#Lung Opacity \n",
    "print(\"肺野の透過性低下\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[103,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[104,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[108,:][\"Path\"]))\n",
    "\n",
    "#Lung Lesion \n",
    "print(\"肺野病変\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[457,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[494,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[554,:][\"Path\"]))\n",
    "\n",
    "#Edema \n",
    "print(\"肺水腫\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[295,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[314,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[387,:][\"Path\"]))\n",
    "\n",
    "#Consolidation \n",
    "print(\"肺野浸潤影\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[557,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[1008,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[1015,:][\"Path\"]))\n",
    "\n",
    "#Pneumonia \n",
    "print(\"肺炎\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[716,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[1037,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[1662,:][\"Path\"]))\n",
    "\n",
    "#Atelectasis \n",
    "print(\"無気肺\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[54,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[196,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[526,:][\"Path\"]))\n",
    "\n",
    "#Pneumothorax \n",
    "print(\"気胸\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[120,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[1513,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[1928,:][\"Path\"]))\n",
    "\n",
    "#Pleural Effusion \n",
    "print(\"胸水\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[330,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[392,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[436,:][\"Path\"]))\n",
    "\n",
    "#Pleural Other \n",
    "print(\"胸膜病変その他\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[4891,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[6112,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[8861,:][\"Path\"]))\n",
    "\n",
    "#Fracture \n",
    "print(\"骨折\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[2404,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[2690,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[3130,:][\"Path\"]))\n",
    "\n",
    "#Support Devices \n",
    "print(\"サポートデバイス\")\n",
    "glance(os.path.join(path, labels_train_raw.iloc[227,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[293,:][\"Path\"]))\n",
    "glance(os.path.join(path, labels_train_raw.iloc[406,:][\"Path\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理  \n",
    "正面像のみを抽出し、画像は256x256pxに整形した  \n",
    "ラベルの欠損値は0で補完し、uncertainも0とした(かなり雑)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#データ前処理\n",
    "ROOT = \"path/to\" #CheXpert-v1.0-smallのディレクトリのある場所へのフルパス\n",
    "\n",
    "labels = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "          'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "          'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "          'Support Devices'] #label項目を設定\n",
    "\n",
    "def read_pics_process(path_df):\n",
    "    \"\"\"画像及びラベル読み込み並列実行用関数\"\"\"\n",
    "    img = scipy.misc.imread(os.path.join(ROOT,path_df[0]))\n",
    "    img = scipy.misc.imresize(img, (256, 256))\n",
    "    img = img.reshape(1, 256, 256, 1)\n",
    "    \n",
    "    num = path_df[1].loc[:,\"Path\"].tolist().index(path_df[0])\n",
    "    label = path_df[1].loc[num,labels]\n",
    "        \n",
    "    print(\"filename:\", path_df[0])\n",
    "    return [img,label]\n",
    "\n",
    "def preprocessing(path):    \n",
    "    \"\"\"正面画像のラベルデータと画像のあるパスを抽出＋欠測値補完\"\"\"\n",
    "    #csvファイルの読み込み\n",
    "    df = pd.read_csv(path)\n",
    "    #正面像のデータだけcsvファイルから抽出\n",
    "    df_frontal = df[df[\"Frontal/Lateral\"]==\"Frontal\"].reset_index(drop=True)\n",
    "    #ラベルデータの負数と欠測値を0に置換\n",
    "    df_frontal[labels] = df_frontal[labels].fillna(0)\n",
    "    df_frontal[labels] = df_frontal[labels].replace(-1, 0)\n",
    "    \n",
    "    #正面像画像のパスをcsvファイルから抽出\n",
    "    img_path = df[df[\"Frontal/Lateral\"]==\"Frontal\"].loc[:,\"Path\"].tolist()\n",
    "    \n",
    "    return df_frontal, img_path\n",
    "\n",
    "def save_img_arr(results,name):\n",
    "    \"\"\"画像データのnumpyアレイを保存する関数\"\"\"\n",
    "    img_arr_tmp = [x[0] for x in results] \n",
    "    img_arr = np.concatenate(img_arr_tmp,axis=0)\n",
    "    np.save(name,img_arr)\n",
    "\n",
    "def save_label_csv(results,name):\n",
    "    \"\"\"ラベルデータを保存する関数\"\"\"\n",
    "    labels_df_tmp = [x[1] for x in results] \n",
    "    labels_df = pd.concat([pd.DataFrame(x) for x in labels_df_tmp],axis=1).T\n",
    "    labels_df.to_csv(name,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#以下前処理実行\n",
    "#csvファイルへのフルパス作成\n",
    "path_train_csv = os.path.join(ROOT,\"CheXpert-v1.0-small/train.csv\")\n",
    "path_valid_csv = os.path.join(ROOT,\"CheXpert-v1.0-small/valid.csv\")\n",
    "\n",
    "#欠測値補完したcsvファイルと正面画像のラベルデータと画像のあるパスを得る\n",
    "df_train_frontal, img_path_train = preprocessing(path_train_csv)\n",
    "df_valid_frontal, img_path_valid = preprocessing(path_valid_csv)\n",
    "\n",
    "#画像取り込み開始(train data[:3000]) *3000枚だけ\n",
    "n_jobs=multi.cpu_count()\n",
    "arg_t = [(i, df_train_frontal) for i in img_path_train[:3000]]\n",
    "p = Pool(n_jobs)\n",
    "train_data = p.map(read_pics_process, arg_t)\n",
    "p.close()\n",
    "\n",
    "#画像取り込み開始(valid data)\n",
    "arg_v = [(i, df_valid_frontal) for i in img_path_valid]\n",
    "p = Pool(n_jobs)\n",
    "valid_data = p.map(read_pics_process, arg_v)\n",
    "p.close()\n",
    "\n",
    "#得たデータをそれぞれ保存\n",
    "save_img_arr(train_data,\"Chexpert_small_train_small\")\n",
    "save_img_arr(valid_data,\"Chexpert_small_valid\")\n",
    "save_label_csv(train_data,\"Chexpert_small_train_label_small.csv\")\n",
    "save_label_csv(valid_data,\"Chexpert_small_valid_label.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## とりあえず試しにディープラーニングで分類してみる  \n",
    "KerasでImageNetの重みの載ったXception読み込んでmulti-labelのclass分類してみた  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Xception読み込み用\"\"\"\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_shape=(256,256,3))\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    \n",
    "    outputs=[]\n",
    "    for i in range(14):\n",
    "        exec(\"output\" + str(i).zfill(2) + \" = Dense(2, activation='softmax', name='output\" + str(i).zfill(2) + \"')(x)\")\n",
    "        exec(\"outputs.append(\" + \"output\" + str(i).zfill(2) + \")\")  \n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    loss_dict = {}\n",
    "    for i in range(14):\n",
    "        exec(\"loss_dict['output\"+str(i).zfill(2)+\"'] = 'binary_crossentropy'\")\n",
    "             \n",
    "    model.compile(optimizer='rmsprop', \n",
    "                  loss=loss_dict,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def preprocess_input(x):\n",
    "    return ((x/255.)-0.5)*2.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#データの読み込みと整形\n",
    "labels_train = pd.read_csv(\"Chexpert_small_train_label_small.csv\")\n",
    "labels_valid = pd.read_csv(\"Chexpert_small_valid_label.csv\")\n",
    "X_train = preprocess_input(np.load(\"Chexpert_small_train_small.npy\"))\n",
    "X_valid = preprocess_input(np.load(\"Chexpert_small_valid.npy\"))\n",
    "X_train = np.tile(X_train,(1,1,1,3))\n",
    "X_valid = np.tile(X_valid,(1,1,1,3))\n",
    "\n",
    "y_dict_train = {}\n",
    "y_dict_valid = {}\n",
    "\n",
    "for i in range(14):\n",
    "    exec(\"y_train\" + str(i).zfill(2) + \" = keras.utils.to_categorical(labels_train.iloc[:,\" + str(i) + \"], 2)\")\n",
    "    exec(\"y_valid\" + str(i).zfill(2) + \" = keras.utils.to_categorical(labels_valid.iloc[:,\" + str(i) + \"], 2)\")\n",
    "    exec(\"y_dict_train['output\" + str(i).zfill(2) + \"'] = y_train\" + str(i).zfill(2))\n",
    "    exec(\"y_dict_valid['output\" + str(i).zfill(2) + \"'] = y_valid\" + str(i).zfill(2))\n",
    "    \n",
    "\n",
    "#kerasでXceptionを読み込んで学習実行\n",
    "model = build_model()\n",
    "\n",
    "es_cb = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "cp_cb = ModelCheckpoint(filepath = 'chexpert_model.styles.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "batch_size=32\n",
    "nb_epoch=100\n",
    "    \n",
    "history = model.fit(X_train, y_dict_train, batch_size=batch_size, shuffle=True, epochs=nb_epoch, \n",
    "                    validation_data=(X_valid, y_dict_valid), callbacks=[cp_cb, es_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loss\n",
    "plt.plot(history.history['loss'],\"o-\",label=\"loss\",)\n",
    "plt.plot(history.history['val_loss'],\"o-\",label=\"val_loss\")\n",
    "plt.title('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loss](https://raw.githubusercontent.com/sekips/chexpert_try/master/loss_3000.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ラベルごとのloss\n",
    "fig = plt.figure(figsize=(16,20))\n",
    "axes = [] \n",
    "for i in range(14):\n",
    "    axes.append(fig.add_subplot(5, 3, i+1))\n",
    "    exec(\"axes[i].plot(history.history['output\" + str(i).zfill(2) + \"_loss'],'o-',label='\" + labels[i] + \"_loss')\")    \n",
    "    exec(\"axes[i].plot(history.history['val_output\" + str(i).zfill(2) + \"_loss'],'o-',label='val_\" + labels[i] + \"_loss')\")\n",
    "    axes[i].legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![each_loss](https://raw.githubusercontent.com/sekips/chexpert_try/master/each_loss_3000.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ラベルごとのaccuracy\n",
    "fig = plt.figure(figsize=(16,20))\n",
    "axes = [] \n",
    "for i in range(14):\n",
    "    axes.append(fig.add_subplot(5, 3, i+1))\n",
    "    exec(\"axes[i].plot(history.history['output\" + str(i).zfill(2) + \"_acc'],'o-',label='\" + labels[i] + \"_acc')\")    \n",
    "    exec(\"axes[i].plot(history.history['val_output\" + str(i).zfill(2) + \"_acc'],'o-',label='val_\" + labels[i] + \"_acc')\")\n",
    "    axes[i].set_ylim([0, 1])\n",
    "    axes[i].legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![each_acc](https://raw.githubusercontent.com/sekips/chexpert_try/master/each_acc_3000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 試しに全データで学習\n",
    "#### Loss\n",
    "![each_acc](https://raw.githubusercontent.com/sekips/chexpert_try/master/model_loss_all.png)\n",
    "#### ラベルごとのloss\n",
    "![each_acc](https://raw.githubusercontent.com/sekips/chexpert_try/master/each_loss_all.png)\n",
    "#### ラベルごとのaccuracy  \n",
    "![each_acc](https://raw.githubusercontent.com/sekips/chexpert_try/master/each_acc_all.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
